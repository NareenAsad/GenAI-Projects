# ğŸ¤– AI Chat Assistant - Local LLM Interface

A beautiful, modern chat interface for interacting with locally installed Large Language Models (LLMs) using Ollama and Streamlit.

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-1.31+-red.svg)
![Ollama](https://img.shields.io/badge/Ollama-Latest-green.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

## âœ¨ Features

- ğŸ¨ **Modern Glass-morphism UI** - Beautiful gradient background with frosted glass effects
- ğŸ’¬ **Real-time Chat Interface** - Smooth, responsive chat experience
- ğŸ¤– **Local LLM Support** - Run AI models completely offline via Ollama
- ğŸ“œ **Conversation History** - Keep track of all your messages
- ğŸ¯ **Simple & Clean** - Minimalist design, easy to use
- âš¡ **Fast & Lightweight** - Optimized performance
- ğŸ”’ **Privacy First** - All data stays on your machine

## ğŸ“¸ Screenshots

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¤– AI Chat Assistant                   â”‚
â”‚  ğŸ¦™ Powered by Ollama - Using: llama2  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚  ğŸ‘¤ User: Hello, how are you?          â”‚
â”‚                                         â”‚
â”‚  ğŸ¤– Bot: I'm doing well, thanks!       â”‚
â”‚      How can I help you today?         â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  [Type your message here...]  [Send]   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8 or higher
- Ollama installed on your system

### Installation

1. **Clone or download this repository**
   ```bash
   mkdir streamlit_llm_app
   cd streamlit_llm_app
   ```

2. **Create a virtual environment**
   ```bash
   # Windows
   python -m venv venv
   venv\Scripts\activate

   # Mac/Linux
   python3 -m venv venv
   source venv/bin/activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Install Ollama**
   - Download from: [https://ollama.ai](https://ollama.ai)
   - Follow installation instructions for your OS

5. **Pull the LLM model**
   ```bash
   ollama pull llama2
   ```

6. **Run the application**
   ```bash
   # Start Ollama server (in one terminal)
   ollama serve

   # Start Streamlit app (in another terminal)
   streamlit run app.py
   ```

7. **Open your browser**
   - Navigate to: `http://localhost:8501`

## ğŸ“ Project Structure

```
streamlit_llm_app/
â”‚
â”œâ”€â”€ app.py                 # Main application file
â”œâ”€â”€ style.css              # Custom styling
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ README.md             # This file
â””â”€â”€ venv/                 # Virtual environment (not committed)
```

## ğŸ› ï¸ Configuration

### Change the Default Model

Edit `app.py` and modify this line:

```python
DEFAULT_MODEL = 'llama2'  # Change to your preferred model
```

Available models you can use:
- `llama2` - General purpose (7B)
- `mistral` - Fast and efficient
- `codellama` - For coding tasks
- `phi` - Lightweight model
- `gemma` - Google's model

Pull any model with:
```bash
ollama pull <model-name>
```

## ğŸ“¦ Dependencies

```
streamlit>=1.31.0
requests>=2.31.0
```

Install all dependencies:
```bash
pip install -r requirements.txt
```

## ğŸ¯ Usage

1. **Start a conversation**
   - Type your message in the input box at the bottom
   - Press Enter or click "Send"

2. **Clear chat history**
   - Click the "Clear Chat" button in the top-right corner

3. **View responses**
   - User messages appear with a ğŸ‘¤ avatar
   - Bot responses appear with a ğŸ¤– avatar

## ğŸ”§ Troubleshooting

### "Connection Error: Make sure Ollama is running"

**Solution:** Start the Ollama server
```bash
ollama serve
```

### "Model not found"

**Solution:** Pull the model first
```bash
ollama pull llama2
```

### "CSS file not found"

**Solution:** Ensure `style.css` is in the same directory as `app.py`

### Port already in use

**Solution:** Run on a different port
```bash
streamlit run app.py --server.port 8502
```

### Slow responses

**Solution:** Try a smaller model
```bash
ollama pull phi
# Then change DEFAULT_MODEL to 'phi' in app.py
```

## ğŸŒŸ Features in Detail

### Chat Interface
- Clean, modern design with glass-morphism effects
- Smooth animations and transitions
- Auto-scrolling chat history
- Fixed input area at bottom

### Model Integration
- Communicates with Ollama via REST API
- Supports all Ollama-compatible models
- Handles connection errors gracefully
- Timeout protection (120 seconds)

### User Experience
- Form-based input (press Enter to send)
- Loading spinner during processing
- Clear error messages
- Conversation persistence during session

## ğŸ¨ Customization Guide

### Change Colors

**Purple Theme (Default):**
```css
background: linear-gradient(to bottom right, #1a1a2e, #16213e, #0f3460);
```

**Blue Theme:**
```css
background: linear-gradient(to bottom right, #0f2027, #203a43, #2c5364);
```

**Green Theme:**
```css
background: linear-gradient(to bottom right, #134e5e, #71b280);
```

### Modify Layout

Change the chat container height in `style.css`:
```css
.chat-container {
    max-height: 600px;  /* Adjust this value */
}
```

## ğŸ¤ Contributing

Contributions are welcome! Here's how you can help:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- [Streamlit](https://streamlit.io/) - For the amazing web framework
- [Ollama](https://ollama.ai/) - For making local LLMs accessible
- [Meta AI](https://ai.meta.com/) - For the Llama 2 model

## ğŸ“§ Contact

Have questions or suggestions? Feel free to:
- Open an issue
- Submit a pull request
- Contact the maintainer

## ğŸ”® Future Enhancements

- [ ] Multiple model support with dropdown selector
- [ ] Export chat history to file
- [ ] Streaming responses (token-by-token)
- [ ] Voice input support
- [ ] Dark/Light theme toggle
- [ ] Custom system prompts
- [ ] Chat history persistence (save to database)
- [ ] Multi-language support

## ğŸ“š Additional Resources

- [Streamlit Documentation](https://docs.streamlit.io)
- [Ollama Documentation](https://github.com/ollama/ollama)
- [Llama 2 Model Card](https://github.com/facebookresearch/llama)

---

**Made with â¤ï¸ using Streamlit and Ollama**

*Star â­ this repository if you find it helpful!*