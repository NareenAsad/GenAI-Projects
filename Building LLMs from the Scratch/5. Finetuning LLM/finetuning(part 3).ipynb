{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d0bd9cd",
   "metadata": {},
   "source": [
    "## **5) Instruction finetuning (part 3; benchmark evaluation)**\n",
    "\n",
    "* In the previous notebook, we finetuned the LLM; in this notebook, we evaluate it using popular benchmark methods\n",
    "\n",
    "* There are 3 main types of model evaluation\n",
    "    - MMLU-style Q&A\n",
    "    - LLM-based automatic scoring\n",
    "    - Human ratings by relative preference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9e770f",
   "metadata": {},
   "source": [
    "**Types of Model Evaluation**\n",
    "\n",
    "| **Type** | **Description** | **Example Use Case** |\n",
    "|-----------|------------------|-----------------------|\n",
    "| **MMLU-style Q&A** | Evaluates model accuracy on multiple-choice or factual question-answering tasks across various subjects. | Testing general knowledge or reasoning ability using MMLU benchmark datasets. |\n",
    "| **LLM-based Automatic Scoring** | Uses another large language model as a judge to automatically score or compare model responses. | GPT-4 scoring the quality or relevance of generated answers. |\n",
    "| **Human Ratings by Relative Preference** | Human evaluators compare outputs from multiple models and rate which one performs better. | Crowdsourced evaluations to measure helpfulness, coherence, or creativity. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001c2292",
   "metadata": {},
   "source": [
    "**5.2 Evaluation**\n",
    "\n",
    "* In this notebook, we do an MMLU-style evaluation in LitGPT, which is based on the EleutherAI LM Evaluation Harness\n",
    "\n",
    "* There are hundreds if not thousands of benchmarks; using the command below, we filter for MMLU subsets, because running the evaluation on the whole MMLU dataset would take a very long time\n",
    "\n",
    "* Let's say we are intrested in the mmlu_philosophy subset, se can evaluate the LLM on MMLU as follows\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64620d92",
   "metadata": {},
   "source": [
    "## **Exercise 3: Evaluate the finetuned LLM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e05a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "!litgpt evaluate out/finetune/lora/final --tasks \"mmlu_philosophy\" --batch_size 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
